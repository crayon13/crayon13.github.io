---
layout: post
title: "엘라스틱서치 실무 가이드"
category: books
tags: 
    - "elastic search"
    - 개발
comments: true
---
# 1. 검색 시스템 이해하기
## 1.1 검색 시스템의 이해
### 1.1.1 검색 시스템이란?
`검색 서비스 > 섬색 시스탬 > 검색 엔진`

엘라스틱서치는 검색엔진이다.
### 1.1.2. 검색 시스템의 구성요소
- 스토리지
- 색인기
- 검색기

### 1.1.3. 관계형 데이터베이스와의 차이점
**구조와용어의 비교**

엘라스틱서치 | 관계형 데이터베이스
-----| ----
인덱스 (Index) | 데이터베이스 (Database)
샤드 (Shard) | 파티션 (Partition)
타입 (Type) | 테이블 (Table)
문서 (Document) | 행 (Row) -> Record가 더 적당할 듯
필드 (Field) | 열 (Column)
매핑 (Mapping) | 스키마 (Schema)
Query DSL | SQL

**추가, 검색, 삭제, 수정 기능 비교**
- 엘라스틱서치는 RESTful API를 사용

HTTP 메서드 | 기능 | 데이터베이스 질의 문법
----|----|---- 
GET | 데이터 조회 | SELECT
PUT | 데이터 생성 | INSERT
POST | 데이터 업데이트, 조회 | UPDATE, SELECT
DELETE | 데이터 삭제 | DELETE
HEAD | 인덱스의 정보 확인 | - 


엘라스틱서치 API 구조
```
curl -X (메서드) http://host:port/(인덱스)/(타입)/(문서 id) -d '{json 데이터}'
```
DBMS와 비교해 보자
```
SELECT * FROM user WHERE name like '%가마돈%'

GET http://localhost:9200/user/_search?q=Name:가마돈
```
결과는 다음과 같이
```json
{
    "Id":1,
    "Name" : "가 마 돈", "Location": "서울", 
    "Gender" : "남",
    "Date":"2018-05-12"
}
```

## 1.2 검색 시스템과 엘라스틱서치
### 1.2.1 엘라스틱서치가 강력한 이유
- 오픈소스 검색엔진
- 전문 검색 (Full Text Search)
- 통계 분석
- 스키마리스 (Schemaless)
    - 데이터베이스는 스키마라는 구조에 따라 데이터를 적합한 형태로 변경해서 저장하고 관리한다.
    - 반면 엘라스틱서치는 정형화되지 않은 다양한 형태의 문서도 자동으로 색인하고 검색할 수 있다.
- RESTful API
- 멀티테넌시 (Multi-tenancy)
    - 서로 상이한 인덱스일지라도 검색할 필드명만 같으면 여러개의 인덱스를 한번에 조회 할 수 있다. 이를 이용해 멀티테넌시 기능을 제공할 수 있다.
- Document-Oriented
    - 여러 계증의 데이터를 JSON 형식의 구조화된 문서로 인덱스에 저장할 수 있다.
    계층 구조로 문서도 한번의 쿼리로 쉽게 조회 할 수 있다.
- 역색인 (Inverted Index)
    - 엘라스틱서치는 루씬기반으로 역색인을 지원한다.
    - 역색인이란 종이책의 마지막 페이지에서 제공하는 색인 페이지와 비슷하게 제공되는 특수한 데이터 구조다.
- 확장성과 가용성

### 1.2.2 엘라스틱서치의 약점
- 실시간이 아니다.
    - 일반적으로 색인된 데이터는 통상적으로 1초 뒤에나 검색이 가능해 진다.
- 트랜잭션과 롤백 기능을 제공하지 않는다.
- 데이터의 업데이트를 제공하지 않는다.
    - 삭제, 생성의 과정을 거친다.

## 1.3 실습 환경 구축
- 주요 설정

설정 | 설명
--- | ---
cluster.name | 클러스터..
node.name | 1개의 클러스터는 N개의 노드를 가질 수 있다.
path.data | 엘라스틱서치의 인덱스 경로를 지정한다.<br/>설정하지 않으면 기본적으로 엘라스틱서치 하위의 data 디렉토리에 인덱스가 생성된다.
path.logs | 노드와 클러스터에서 생성되는 로그 경로 지정
path.repo | 인덱스를 백업하기 위한 스냅샷 경로를 지정
network.host | 특정 IP만 접근허용 0.0.0.0 은 전체 허용 -> 서버에 할당된 아이피 기준
http.port | 클라인트가 접근할 수 있는 http 포트 번호 - 기본 9200 - RESTFul API 포트입니다.
transport.tcp.port | 클라인트가 접근할 수 있는 TCP 포트 - 기본 9300 - node 간 통신에서 사용하는 포트입니다.
discovery.zen.ping.unicast.hosts | 노드가 N개인 경우 유니캐스트로 활성화된 다른 서버를 찾는다.<br/>클러스터로 묶인 노드(서버)의 IP를 지정하면 된다.<br/>노드가 2개인 경우 `[1.1.1.1, 2.2.2.2]` 형태로 선언
discovery.zen.minimum_master_nodes | 마스터 노드의 선출 기준이 되는 노드의 수를 지정한다.
node.master | 마스터 노드로 동작 여부를 지정한다.
node.data | 데이터 노드로 동작 여부를 지정한다.

# 2. 엘라스틱서치 살펴보기
## 2.1 엘라스틱서치를 구성하는 개념
### 2.1.1 기본 용어
- 전체 구조
    - Index > Type > Document > Field, Field...

- 인덱스 (Index)
    - 인덱스는 데이터 저장 공간이다.
    - 하나의 인덱스는 하나의 타입만 가지며 하나의 물리적인 노드에 여러 개의 논리적 인덱스를 생성 할 수 있다.
    - 검색 시 인덱스 이름으로 문서 데이터를 검색하며, 여러 개의 인덱스를 동시에 검색하는 것도 가능하다.
    - ES를 분산 환경으로 구성하면 하나의 인덱스가 여러 노드에 분산 저장된다.
    - 인덱스 생성 시 기본적으로 5개의 Primary 샤드 1개의 Replica 샤드 세트를 생성한다.
    - 각각의 샤드 수는 인덱스를 생성할 때 옵션으로 변경 할 수 있다.
    - 인덱스 이름은 모두 소문자여야 하며 추가, 수정, 삭제, 검색은 RESTful API로 실행할 수 있다.
    - 만약 인덱스가 없는 상태에서 데이터가 추가도니다면 데이터를 이용해 인덱스가 자동 생성된다.
- 샤드 (Shard)
    - 색인된 문서는 하나의 인덱스에 담긴다.
    - 인덱스 내부에 색인된 데이터는 물리적 공간에 여러 개의 파티션으로 나뉘어 구성되는데, 이 파티션을 엘라스틱서치에서는 샤드라고 부른다.
    - es는 다수의 샤드로 문서를 분산 저장하고 있어 데이터 손실 위험을 최소화 할 수 있다.
- 타입 (Type)
    - 타입은 인덱스의 논리적 구조를 의미하며, 인덱스 속성에 따라 분류하기도 한다.
    - es 6.0 이하 버전에서는 하나의 인덱스에 여러 타입을 설정 가능했지만.
    - 6.1 부터 인덱스당 하나의 타입만 사용할 수 있다.
- 문서 (Document)
    - es에서 데이터가 저장되는 최소 단위다
    - JSON 포맷으로 데이터가 저장된다.
- 필드 (Field)
    - 필드는 문서를 구성하기 위한 속성이라고 할수 있다.
    - 일반적으로 데이터베이서의 컬럼과 비교 할 수 있으나 컬럼이 정적(Static) 인 데이터 타입이라면
    - 필드는 좀 더 동적 (Dynamic)인 데이터 타입이라고 할 수 있다.
    - 하나의 필드는 목적에 따라 다수의 데이터 타입을 가질 수 있다.
        - 영화 제목 필드는 검색할 때 매칭 검색 혹은 초성 검색이 모두 지원되도록 2개의 데이터 타입을 가져야 한다.
- 매핑 (Mapping)
    - 매핑은 문서의 필드와 필드의 속성을 정의하고 그에 따른 색인 방법을 정의하는 프로세스다.
    - 인덱스의 매핑 정보에는 여러 가지 데이터 타입을 지정할 수 있지만 필드명은 중복해서 사용할 수 없다.
    
### 2.1.2 노드의 종류
클러스터는 물리적인 노드 인스턴스들의 모임이다.
클러스터는 모든 노드의 검색과 색인 작업을 관장하는 논리적인 개념.
- 마스터 노드 (Master Node)
    - 노드의 추가와 제거, 인덱스의 생성과 삭제 등 클러스터와 관련된 전반적인 작업을 담당한다.
    - 네트워크 속도가 빠르고 지연이 없는 노드를 마스터 노드로 선정해야 한다.
    - 다수의 노드를 마스터로 설정할 수 있지만 결과적으로 하나의 노드만이 마스터 노드로 선출되어 동작한다.
- 데이터 노드 (Data Node)
    - 실질적인 데이터를 저장한다.
    - 검색과 통계 같은 데이터 관련 작업을 실행한다.
    - 데이터가 실제로 분산 저장되는 물리적 공간인 샤드가 배치되는 노드이기도 하다.
    - 색인 작업은 CPU, 메모리, 스토리지 같은 컴퓨팅 리소스를 많이 소모하기 때문에 리소스 모니터링이 필요하다.
    - 데이터 노드는 가능한 한 마스터 노드와 분리해서 구성하는게 좋다.
- 코디네이팅 노드 (Coordinating Node)
    - 사용자의 요청만 받아서 처리한다.
    - 클러스터 관련 요청은 마스터 노드에 전달하고 데이터 관련 요청은 데이터 노드에 전달한다.
- 인제스트 노드 (Ingest Node)
    - 문서의 전처리 작업을 담당한다.
    - 인덱스 생성 전 문서의 형식을 다양하게 변경 할 수 있다.

**샤드**
    - 프라이머리 샤드와 레플리카 샤드로 구분
    - 장애 발생 시 마스터 노드는 데이터를 재분배하거나 레플리카 샤드를 프라이머리 샤드로 승격시켜 서비스 중단 없는 복구가 가능해진다.

## 2.2 엘라스틱서치에서 제공하는 주요 API
**API의 종류**
- 인덱스 관리 API (Indices API)
- 문서 관리 API (Document API) : 문서의 추가/수정/삭제
- 검색 API (Search API) : 문서 조회
- 집계 API (Aggregation API) : 문서 통계

`스키마리스 기능은 가급적이면 사용하지 말자.`
- 스키마리스는 인덱스를 자동으로 생성하는데 필드 정보가 매핑되지 않아 의도와 다르게 동작할 수 있다.
- 또한 필드에 중복 타입이 지정되어 Standard Analyzer를 사용하게 된다.

`스키마리스 기능을 명시적으로 사용하지 않도록 설정하는 방법`
- 노드 설정 파일에서 action.auto_create_index = false로 정의
- 인덱스 별로 제공되는 index.mapper.dynamic = false 로 설정하여 필드의 자동 매핑 생성을 비활성화

### 2.2.1 인덱스 관리 API
**클러스터 상태 확인**
```
GET /_cat/health?v&pretty

```  
<br/>
**노드 리스트 조회**
```
GET /_cat/nodes?v&pretty
ip        heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
127.0.0.1           29          95   0    0.00    0.01     0.05 mdi       *      6mBIlW0
```  
<br/>
**인덱스 리스트 조회**
```
GET /_cat/indices?v&pretty
health status index uuid pri rep docs.count docs.deleted store.size pri.store.size
```  
<br/>
**Alias 리스트 조회**
```json
GET /_cat/aliases?v
// 결과
alias index filter routing.index routing.search


GET /_alias
// 결과
{
  ".kibana": {
    "aliases": {}
  },
  "movie": {
    "aliases": {}
  },
  "movie_kibana_execute": {
    "aliases": {}
  },
  "movie_search": {
    "aliases": {}
  }
}
```  
<br/>
**인덱스 생성**
- 인덱스를 생성 할 때는 매핑이라는 세부 설정을 이용할 수 있다.
- 매핑은 문서와 문서에 포함된 필드, 필드 타입을 세세하게 지정 가능하다.

```json
PUT /movie
{ 
    "settings" : {
        "number_of_shards" :3,
         "number_of_replicas" :2
    }
},
"mappings" : { 
    "_doc" : {
        "properties" : {
            "movieCd" :{ "type" :"integer" }, 
            "movieNm": { "type" :"text" }, 
            "movieNmEn" : { "type" : "text" },
            "prdtYear" :{ "type1" :"integer" },
            "openDt" : { "type" : "date" }, 
            "typeNm" : { "type" : "keyword" }, 
            "prdtStatNm" : { "type" : "keyword" }, 
            "nationAlt":{ "type" : "keyword" }, 
            "genreAlt" :{ "type" :"keyword" }, 
            "repNationNra" : { "type" : "keyword" }, 
            "repGenreNm" : { "type" : "keyword" }
        } 
    }
}
```
- 결과는
```json
{
    "acknowledged": true,
    "shards_acknowledged": true,
    "index": "movie"
}
```

**인덱스 삭제**
```
DELETE /movie
```
- 결과는
```json
{
    "acknowledged": true
}
```
- 인덱스 이름이 잘못되었거나 없을 경우는
```json
{
    "error": {
        "root_cause": [
            {
                "type": "index_not_found_exception",
                "reason": "no such index",
                "resource.type": "index_or_alias",
                "resource.id": "movie",
                "index_uuid": "_na_",
                "index": "movie"
            }
        ],
        "type": "index_not_found_exception",
        "reason": "no such index",
        "resource.type": "index_or_alias",
        "resource.id": "movie",
        "index_uuid": "_na_",
        "index": "movie"
    },
    "status": 404
}
```
`인덱스는 한번 삭제하면 복구 할 수 없다!!`

### 2.2.2 문서관리 API
문서관리 API는 실제 문서를 색인하고 조회, 수정, 삭제를 지원하는 API다.
- `Index API` : 한 건의 문서를 색인한다.
- `Get API` : 한 건의 문서를 조회한다.
- `Delete API` : 한 건의 문서를 삭제한다.
- `Update API` : 한 건의 문서를 업데이트 한다 -> es는 업데이트라는 개념이 없으므로 삭제 후 추가로 실행 됩니다.

문서 관리 API는 한 건의 문서를 처리하는 기능이먀, `Single-document API` 라고도 부른다.
다수의 문서를 처리 할 경우는? `Multi-document API`도 제공한다!!
- `Multi Get API` : 다수의 문서를 조회한다.
- `Bulk API` : 대량의 문서를 색인한다.
- `Delete By Quer API`: 다수의 문서를 삭제한다.
- `Update By Query API` : 다수의 문서를 업데이트 한다.
- `Reindex API`: 인덱스의 문서를 다시 색인한다.

*일반적인 내용이므로 그냥 이런 API가 있다는 것만 알아 놓자.*

**문서 생성**
```json
POST /movie/_doc/1
{ 
    "movieCd" :"1",
    "movieNin" :"살아남은 아이", 
    "movieNmEn":"Last Child", 
    "prdtYear" :"2017", 
    "openDt" :"2017-10-20", 
    "typeNm" :"장편",
    "prdtStatNm" : "기 타",
    "nationAlt" : "한국" , 
    "genreAlt":"드라마,가족", 
    "repNationNm" : "한국", 
    "repGenreNm": "드라마"
}
```
- 결과는
```json
{
    "_index": "movie",
    "_type": "_doc",
    "_id": "1",
    "_version": 1,
    "result": "created",
    "_shards": {
        "total": 2,
        "successful": 1,
        "failed": 0
    },
    "_seq_no": 0,
    "_primary_term": 1
}
```

**문서 조회**
```
GET /movie/_doc/1
```
- 결과는
```json
{
    "_index": "movie",
    "_type": "_doc",
    "_id": "1",
    "_version": 1,
    "found": true,
    "_source": {
        "movieCd": "1",
        "movieNin": "살아남은 아이",
        "movieNmEn": "Last Child",
        "prdtYear": "2017",
        "openDt": "2017-10-20",
        "typeNm": "장편",
        "prdtStatNm": "기타",
        "nationAlt": "한국",
        "genreAlt": "드라마,가족",
        "repNationNm": "한국",
        "repGenreNm": "드라마"
    }
}
```

**문서 삭제**
```
DELETE /movie/_doc/1
```
- 결과는
```json
{
    "_index": "movie",
    "_type": "_doc",
    "_id": "1",
    "_version": 2,
    "result": "deleted",
    "_shards": {
        "total": 2,
        "successful": 1,
        "failed": 0
    },
    "_seq_no": 1,
    "_primary_term": 1
}
```

**id를 지정하지 않고 문서 생성**
```json
POST /movie/_doc
{ 
    "movieCd" :"1",
    "movieNin" :"살아남은 아이", 
    "movieNmEn":"Last Child", 
    "prdtYear" :"2017", 
    "openDt" :"2017-10-20", 
    "typeNm" :"장편",
    "prdtStatNm" : "기타",
    "nationAlt" : "한국" , 
    "genreAlt":"드라마,가족", 
    "repNationNm" : "한국", 
    "repGenreNm": "드라마"
}
```
- 결과는
```json
{
    "_index": "movie",
    "_type": "_doc",
    "_id": "LOpxnWsB2ZH0EWydX326",
    "_version": 1,
    "result": "created",
    "_shards": {
        "total": 2,
        "successful": 1,
        "failed": 0
    },
    "_seq_no": 0,
    "_primary_term": 1
}
```
`id가 임의로 부여되었다!!`
    - es가 UUID를 통해 무작위로 생성한다.
        - UUID ? : 범용 고유 식별자 (https://ko.wikipedia.org/wiki/범용_고유_식별자)
    - database 기반 데이터를 색인할 경우 잘 쓰지 않을 듯.

### 2.2.3 검색 API
es 검색 API 사용방식은 다음과 같이 크게 두가지로 나뉜다.
- HTTP URI (Uniform Resource Identifier) 형태의 파라미터를 URI에 추가해 검색하는 방법
- RESTful API 방식인 QueryDSL을 사용해 요청 본문(Request Body)에 질의 내용을 추가해 검색하는 방법 : http://www.querydsl.com
```json
GET /movie/_doc/_search?q=prdtYear:2017&pretty=true
{
    "sort" :{ 
        "movieCd" : {
            "order" :"asc"
        }
    }
}
```
- 위 예제대로 하면 다음과 같은 에러가 납니다.
```json
{
    "error": {
        "root_cause": [
            {
                "type": "illegal_argument_exception",
                "reason": "Fielddata is disabled on text fields by default. Set fielddata=true on [movieCd] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead."
            }
        ],
        "type": "search_phase_execution_exception",
        "reason": "all shards failed",
        "phase": "query",
        "grouped": true,
        "failed_shards": [
            {
                "shard": 0,
                "index": "movie",
                "node": "Up2EY-UrSjGccqaNypZanw",
                "reason": {
                    "type": "illegal_argument_exception",
                    "reason": "Fielddata is disabled on text fields by default. Set fielddata=true on [movieCd] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead."
                }
            }
        ],
        "caused_by": {
            "type": "illegal_argument_exception",
            "reason": "Fielddata is disabled on text fields by default. Set fielddata=true on [movieCd] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.",
            "caused_by": {
                "type": "illegal_argument_exception",
                "reason": "Fielddata is disabled on text fields by default. Set fielddata=true on [movieCd] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead."
            }
        }
    },
    "status": 400
}
```
- 요걸 참고 : https://rokking1.blog.me/221366675132
```json
PUT /movie/_mapping/_doc
{
    "properties" : {
        "movieCd" : {
            "type" : "text",
            "fielddata" : true
        }
    }
}
```
- 조회를 하면
```json
{
    "took": 4,
    "timed_out": false,
    "_shards": {
        "total": 3,
        "successful": 3,
        "skipped": 0,
        "failed": 0
    },
    "hits": {
        "total": 2,
        "max_score": null,
        "hits": [
            {
                "_index": "movie",
                "_type": "_doc",
                "_id": "1",
                "_score": null,
                "_source": {
                    "movieCd": "1",
                    "movieNin": "살아남은 아이",
                    "movieNmEn": "Last Child",
                    "prdtYear": "2017",
                    "openDt": "2017-10-20",
                    "typeNm": "장편",
                    "prdtStatNm": "기타",
                    "nationAlt": "한국",
                    "genreAlt": "드라마,가족",
                    "repNationNm": "한국",
                    "repGenreNm": "드라마"
                },
                "sort": [
                    "1"
                ]
            },
            {
                "_index": "movie",
                "_type": "_doc",
                "_id": "L-qhnWsB2ZH0EWydzX0X",
                "_score": null,
                "_source": {
                    "movieCd": "1",
                    "movieNin": "살아남은 아이",
                    "movieNmEn": "Last Child",
                    "prdtYear": "2017",
                    "openDt": "2017-10-20",
                    "typeNm": "장편",
                    "prdtStatNm": "기타",
                    "nationAlt": "한국",
                    "genreAlt": "드라마,가족",
                    "repNationNm": "한국",
                    "repGenreNm": "드라마"
                },
                "sort": [
                    "1"
                ]
            }
        ]
    }
}
```
    - `_shard`에서는 성공적으로 반환한 샤드의 수와 실패한 샤드의 수를 알 수 있다.
        - 검색에 실패한 샤드 수는 검색 시 설정된 `time_out`에 따라 결정된다.
        - `time_out` 시간이 초과되면 그때까지 검색된 내용까지만 결과로 반환된다.
        - 따라서 실패한 샤드의 수가 지나치게 많다면 `time_out` 시간을 조정해야 한다.
    - `hits` 에서는 일치하는 문서의 수와 함께 점수 (_score)가 가장 높은 상위 10개의 문서를 보여준다.

**URI 방식의 검색 질의**

다음과 같이 쿼리스트링만으로  ^^
```
GET /movie/_search?q=장편
```
- 파라미터를 사용할 때 별도의 필드명을 지정하지 않으면 존재하는 모든 필드를 대상으로 검색을 실행한다.
    - 특정 필드만 조회하고 싶다면 다음 코드와 같이 필드명을 포함해서 요청하면 된다.
        ```
        // typeNm 필드의 값이 '장편' 인 문자열 검색
        POST /movie/_search?q=typeNm:장편
        ```

**Request Body 방식의 검색 질의**

URI 검색 질의는 여러 필드를 각기 다른 검색어로 질의 하는 것이 어렵다.
- URI길이 제한 : "RFC 2068 - Hypertext Transfer Protocol -- HTTP/1.1" 에서 정의
    - https://www.rfc-editor.org/rfc/rfc2068.txt
    - >Note: Servers should be cautious about depending on URI lengths
    >above 255 bytes, because some older client or proxy implementations
    >may not properly support these lengths.

기본 구문은 다음과 같습니다.
```json
POST /{index명}/_search 
{
    JSON 쿼리 구문
}

// example
POST /movie/_search
{
    "query" : {
        "term" : {
            "typeNm" : "장편"
        }
    }
}
```
- 왜 POST여야 하는가!!
    - GET이어도 된다!! -> RESTful 하다면 GET이 맞지 않는가?

Request Body의 쿼리 구문은 다음과 같이 여러 개의 키 조합이 가능하다.

Key | 설명 | 기본값
----|----|----
size | `value` 몇 개의 결과를 반환할지 결정한다. | 10
from | `value` 어느 위치부터 반환할지를 결정한다.<br/>0부터 시작하면 상위 0~10건의 데이터를 반환한다. | 0
_source | `value` 특정 필드만 결과로 반환하고 싶을 때 사용한다.<br/>partial response | 
sort | `value` 특정 필드를 기준으로 정렬한다.<br/>asc, desc ... | 
query | `객체`. 검색될 조건을 정의한다. | 
filter | `객체`. 검색 결과 중 특정한 값을 다시 보여준다.<br/>결과 내에서 재검색할 때 사용하는 기능 중 하나다.<br/>다만 필터를 사용하게 되면 자동으로 score 값이 정렬되지 않는다. | 

### 2.2.4 집계 API
과거에는 통계 작업을 위해 루씬이 제공하는 `패싯(Facets)` 기능을 많이 활용했다.
- 11번가는 Solr 를 사용하여 Facets을 쓴다.
- Facets은 DB의 Group By와 동일한 개념이다.  
패싯은 기본적으로 디스크 기반으로 동작했고, 분산 환경에는 최적화 되지 않았기 때문에  
대용량 데이터의 통계 작업에는 적합하지 않았다.

ES는 5.0 이후 패싯 방식의 통계 기능을 제거하고 독자적인 집계 (Aggregation) API를 적용했다.  

>**루씬의 패싯 API**  
>패싯은 같은 항목의 총 개수를 푷시하는 기능으로,
>특정 필드를 기준으로 카테고리화해 총 개수를 취합 후 정렬한 결과를 제공한다.
>패싯 API는 시간 순으로 정렬할 수 없다.

**데이터 집계**
_serch API를 사용해 집계 쿼리를 만들고 terms 키워드를 이용해 genreAlt 필드의 데이터를 그룹화 해 보자.
```json
POST /movie/_search?size=0
{
    "aggs" : {
        "genre" : {
            "terms" : {
                "field" : "genreAlt"
            }
        }
    }
}
```
- 역시 에러가 발생할 것이다... genreAlt 필드에 `fielddata = true` 속성을 적용하자
```json
PUT /movie/_mapping/_doc
{
    "properties" : {
        "movieCd" : {
            "type" : "text",
            "fielddata" : true
        },
        "genreAlt" : {
        	"type" : "text",
        	"fielddata" : true
        }
    }
}
```

- 결과는 다음과 같다.
```json
{
    "took": 25,
    "timed_out": false,
    "_shards": {
        "total": 3,
        "successful": 3,
        "skipped": 0,
        "failed": 0
    },
    "hits": {
        "total": 2,
        "max_score": 0,
        "hits": []
    },
    "aggregations": {
        "genre": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets": [
                {
                    "key": "가족",
                    "doc_count": 2
                },
                {
                    "key": "드라마",
                    "doc_count": 2
                }
            ]
        }
    }
}
```

다음과 같이 버킷안에 다른 버킷의 결과를 추가 할수 있다.
```
POST /movie/_search?size=0
{
    "aggs": {
        "genre": {
            "terms": {
                "field": "genreAlt"
            },
            "aggs": {
                "nation": {
                    "terms": {
                        "field": "nationAlt"
                    }
                }
            }
        }
    }
}
```

**데이터 집계 타입**
집계 기능은 현재 4가지 API로 제공된다. 집계 기능은 서로 조합해서 사용할 수 있다.

집계 방법 | 설명
----|----
버킷 집계 (Bucket Aggregation) | 집계 중 가장 많이 사용한다.<br/>문서의 필드를 기준으로 버킷을 집계한다.
메트릭 집계 (Metric Aggregation) | 문서에 추출된 값을 가지고 Sum, Max, Min, Avg를 계산한다.
메트릭스 집계 (Metrix Aggregation) | 행렬의 값을 합하거나 곱한다.
파이프라인 집계 (Pipeline Aggregation) | 버킷에서 도출된 결과 문서를 다른 필드 값으로 재 분류한다.<br/>즉. 다른 집계에 의해 생성된 출력 결과를 다시 한번 집계한다.<br/>`집계가 패싯보다 강력한 이유가 여기에 있다.` 
<br/> 
<br/> 

# 3. 데이터 모델링
es는 색인할 때 문서의 데이터 유형에 따라 필드에 적절한 데이터 타입을 지정해야 한다.  
이러한 과정을 매핑이라고 하며, 매핑은 색인될 문서의 데이터 모델링이라고도 할 수 있다.  
사전에 매핑을 설정하면 지정된 데이터 타입으로 색인되지만 매핑을 설정해 두지 않으면 es가 자동으로 필드를 생성하고 필드 타입까지 결정한다.  
필드 데이터 타입이 자동으로 지정될 경우 실제 운영환경에서 예상하지 못한 오류가 발생할 수 있기 때문에 매핑 과정은 매우 중요한 과정이다.  
<br/> 

## 3.1 매핑 API 이해하기
매핑은 색인 시 데이터가 어디에, 어떻게 저장될지를 결정하는 설정이다.  
데이터베이스의 스키마에 대응하는 개념이라고도 할 수 있는데,  
인덱스에 추가되는 각 데이터 타입을 구체적으로 정의하는 일이다.  

다음과 같은 2개의 문서가 있다고 하고, 매핑 정의를 살펴 보자.  
```json
# 문서1
{
    "movieCd" : "20173732",
    "movieNm":"캡틴 아메리카"
}
# 문서2 
{
    "movieCd" : "XTQQI",
    "movieNm" :"아이언맨"
}
```
- 문서1 색인시 매핑설정이 없어 movieCd는 `숫자` 타입이 된다.
- 문서2 색인시 movieCd가 `문자` 라 오류가 발생한다.
- 즉, 여러 타입이 혼재된 필드의 경우 색인 오류가 발생 될 수 있다.  

매핑 정보를 설정 할 경우 다음 사항을 고민해야 한다.
> - 문자열을 분석할 것인가?
> - _source에 어떤 필드를 정의할 것인가?
> - 날짜 필드를 가지는 필드는 무엇인가?
> - 매핑에 정의되지 않고 유입되는 필드는 어떻게 처리할 것인가?
<br/>

### 3.1.1 매핑 인덱스 만들기
영화 정보를 다음과같은 매핑 설정으로 만들어 보자
매핑명 | 필드명 | 필드 타입
---|---|---
인덱스 키 | movieCd | keyword
영화제목_국문 | movieNm | text
영화제목_영문 | movieNmEn | text
제작연도 | prdtYear | integer
개봉연도 | openDt | integer
영화유형 | typeNm | keyword
제작상태 | prdtStatNm | keyword
제작국가(전체) | nationAlt | keyword
장르(전체) | genreAlt | keyword
대표 제작국가 | repNationNm | keyword
대표 장르 | repGenreNm | keyword
영화 감독명 | directors.peopleNm | object -> keyword
제작사 코드 | companies.companyCd | object -> keyword
제작사 명 | companies.companyNm | object -> keyword

색인 데이터는 다음과 같다.
```json
{
    "movieCd": "20173732", 
    "movieNm": "살아남은 아이", 
    "movieNmEn":"Last Child", 
    "prdtYear": "2017", 
    "openDt" : "",
    "typeNm":"장편", 
    "prdtStatNm": "기타", 
    "nationAlt" :"한국", 
    "genreAlt":"드라마,가족", 
    "repNationNm": "한국", 
    "repGenreNm":"드라마", 
    "directors" :[{
        "peopleNm": "신동석"
    }],
    "companies" :[{
        "companyCd": "", 
        "companyNm": ""
    }]
}
```
- `text` 타입은 검색 가능한 속성이다.
- `keyword` 타입은 검색 불가능한 속성이다.

매핑 설정은 다음과 같다.
```json
PUT movie_search 
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 1
  },
  "mappings": {
    "_doc": {
      "properties": {
        "movieCd": {
          "type": "keyword"
        },
        "movieNm": {
          "type": "text",
          "analyzer": "standard"
        },
        "moviNmEn": {
          "type": "text",
          "analyzer": "standard"
        },
        "prdtYear": {
          "type": "integer"
        },
        "openDt": {
          "type": "integer"
        },
        "typeNm": {
          "type": "keyword"
        },
        "prdtStatNm": {
          "type": "keyword"
        },
        "nationAlt": {
          "type": "keyword"
        },
        "genreAlt": {
          "type": "keyword"
        },
        "repNationNm": {
          "type": "keyword"
        },
        "repGenreNm": {
          "type": "keyword"
        },
        "companies": {
          "properties": {
            "companyCd": {
              "type": "keyword"
            },
            "companyNm": {
              "type": "keyword"
            }
          }
        },
        "directors": {
          "properties": {
            "peopleNm": {
              "type": "keyword"
            }
          }
        }
      }
    }
  }
}
```

결과는
```json
{
    "acknowledged": true,
    "shards_acknowledged": true,
    "index": "movie_search"
}
```

### 3.1.2 매핑 확인
```
GET movie_search/_mapping
```

### 3.1.3 매핑 파라미터
매핑 파라미터는 색인할 필드의 데이터를 어떻게 저장할지에 대한 다양한 옵션을 제공한다. 

__analyzer__  
> 형태소 분석을 하기 위해 설정한다. <br/>색인과 검색시 지정한 분석기로 형태소 분석을 실행한다.<br/>`text` 타입은 `analyzer` 매핑 파라미터를 기본으로 사용해야 한다.<br/>기본값은 Standard Analyzer 이다.<br/>

__normalizer__  
> term query에 분석기를 사용하기 위해 설정한다.<br/> 예를 들어 `keyword` 타입의 경우 원문을 기준으로 문서가 색인되기 때문에 cafe, Cafe 는 서로 다른 문서로 인식된다.<br/> normalizer > asciifolding과 같은 필터를 사용하면 같은 데이터로 인식되게 할 수 있다.  <br/>

__boost__  
> 필드에 가중치(`weight`)를 부여한다. <br/>가중치에 따라 유사도 점수(`_scope`)가 달라지기 때문에 boost 설정 시 검색 결과의 노출 순서에 영향을 준다.<br/>만약 색인 시점에 boost 설정을 하게되면 재색인시에만 반영이 된다.<br/>`가급적 검색 시점에만 사용하는 것을 권장한다.`<br/>최신 es는 색인 시 boost 설정을 할 수 없도록 바뀌었다. 루씬 7.0 부터 제거 되었기 때문에....  <br/>

__coerce__  
> 색인 시 자동 변환을 허용할지 여부를 설정하는 파라미터다.<br/>예를 들어 "10"과 같은 숫자 형태의 문자열이 integer 타입의 필드에 들어온다면, es는 자동으로 형변환을 하여 정상처리하려 한다.<br/>하지만 coerce 설정을 미사용으로 변경한다면 색인에 실패할 것이다.  <br/>

__copy_to__  
> 매핑 파라미터를 추가한 필드의 값을 지정한 필드로 복사한다.<br/>keyword 타입의 필드에 `copy_to` 로 다른 필드로 복사하고, 복사된 필드에 text 타입을 지정헤 형태소 분석을 할 수도 있다.<br/>또한 여러개의 필드 데이터를 하나의 필드에 모아서 전체 검색 용도로 사용하기도 한다.<br/>과거에 존재하던 _all 필드와 동일한 기능을 제공할 수 있다.  <br/>

__fielddata__  
> es가 힙 공간에 생성하는 메모리 캐시다.<br/>과거에는 `fielddata`를 많이 사용했지만 반복적인 메모리 부족 현상과 잦은 GC로 현재는 거의 사용되지 않는다.<br/>최신 버전의 es는 `doc_values`라는 새로운 캐시를 제공하고 있으며, text타입의 필드를 제외한 모든 필드는 기본적으로 doc_values 캐시를 사용한다.<br/><br/>fielddata를 사용해야만 하는 경우도 있다.<br/>`text타입의 필드는 기본적으로 분석기에 의해 형태소 분석이 되기 때문에 집계나 정렬등의 기능을 실행할 수 없다.` <br/> 하지만 부득이 하게 text타입의 필드에서 집계나 정렬을 실행할 경우 fielddata를 사용해야 한다.<br/> fielddata는 메모리에 생성되는 캐시이기 때문에 최소한으로만 사용해야 한다.<br/>기본값은 비활성화 이다. -> 메모리 소모가 크기 때문에.  <br/>

__doc_values__  
> es에서 사용하는 기본 캐시다.<br/>text타입을 제외한 모든 타입에서 기본적으로 `doc_values` 캐시를 사용한다.<br/>doc_values는 루씬을 기반으로 하는 캐시 방식이다.<br/>

__dynamic__
> 매핑에 필드를 추가할 때 동적으로 생성할지, 생성하지 않을지를 결정한다.  
> 동적 생성 필드의 처리 방법으로 다음 세가지중 하나를 지정할 수 있다.

인자 | 설명
--- | ---
true | 새로 추가되는 필드를 매핑에 추가한다.
false | 새로 추가되는 필드를 무시한다.<br/>해당 필드는 색인되지 않아 검색할 수 없지만 _source에는 표시된다.
strict | 새로운 필드가 감지되면 예외가 발생하고 문서 자체가 색인되지 않는다.<br/>새로 유입되는 필드는 사용자가 매핑에 명시적으로 추가해야 한다. 

__enabled__
> 검색 결과에는 포함하지만 색인은 하지 않을 경우 사용한다.  
> 메타 성격의 데이터에 자주 사용.  
> 예를 들어 게시판에서 제목과 요약 글만 색인하고 날짜와 사용자 ID는 색인하지 않는 경우,  
> enable = false 로 설정하면 _source에는 검색이 되지만 색인은 하지 않는다.  

__format__
> es는 날짜/시간을 문자열로 표시한다.
> 정의된 포맷을 사용 가능하다.
> [https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html]

포맷 | 날짜 형식 | 비고
---|---|---
basic_date | yyyyMMdd | 년도/월/일
basic_date_time | yyyyMMdd'T'HHmmss.SSSZ | 년도/월/일/T/시/분/초/밀리초/Z
basic_time | HHmmss.SSSZ | 시/분/초/밀리초/Z
date<br/>strict_date | yyyy-MM-dd | 년도/시/분
date_hour_minute_second<br/>strict_date_hour_minute_second | yyyy-MM-dd'T'HH:mm:ss. | 년도/시/분T/시/분/초
date_hour_minute_second_millis<br/>strict_date_hour_minute_second_millis | 년도/시/분T/시/분/초/밀리초
date_time<br/>strict_date_time | yyyy-MM-dd'T'HH:mm:ss.SSS.zz | 년도/시/분/T/시/분/초/밀리초/ZZ

__ignore_above__  
> 필드에 저장되는 문자열이 지정한 크기를 넘어서면 빈 값으로 색인.  
> 지정한 크기만큼 색인되는 것이 아니라 빈 값으로 저장되므로 주의한다.

__ignore_malformed__
> es는 잘못된 데이터를 색인하려고 하면 예외가 발생하고, 해당 문서 전체가 색인되지 않는다.  
> 이 매핑 파라미터를 설정하면 해당 필드만 무시하고 색인한다.  

__index__
> 필드값을 색인할지를 결정한다.  
> 기본값은 true 이며, false로 변경하면 해당 필드를 색인하지 않는다.  

__fields__
> 다중 필드(multi_field)를 설정할 수 있는 옵션이다.  
> 필드 안에 또 다른 필드의 정보를 추가 할 수 있어, 같은 String 값을 각각 다른 분석기로 처리하게 설정 할 수 있다.  
> 다음과 같이 기본 필드는 전문 검색을, 추가 필드는 집계용으로 사용 할 수 있다.  

```json
PUT /movie_search/_mapping
{
    "mappings" : {
        "_doc" : {
            "properties" : {
                "awards" : {
                    "type" : "text",
                    "fields" : {
                        "name" : {
                            "type" : "keyword"
                        }
                    }
                }
            }
        }
    }
}
```

__position_increment_gap__  
> 배열 형태의 데이터를 색인 할 때 검색의 정확도를 높이기 위해 제공하는 옵션.  
> 필드 데이터 중 단어와 단어 사이의 간격(slop)을 허용할지를 설정한다.  
> 검색 시 단어와 단어 사이의 간격을 기준으로 일치하는 문서를 찾는데 필요하다.  
> 예를 들어, 데이터가 ["John Abraham", "Lincon Smith"] 일 때 "Abraham Lincon" 으로도 검색 가능하다.  


__properties__  
> 오브젝트 타입이나 중첩(Nested) 타입의 스키마를 정의할 때 사용되는 옵션으로 필드의 타입을 매핑한다.  
> 오브젝트 플드 및 중첩 필드에는 `properties` 라는 서브 필드가 있다.  
> 이 properties 는 object나 nested를 포함한 모든 데이터 타입이 될 수 있다.  

__search_analyzer__  
> 일반적으로는 색인과 검색 시 같은 분석기를 사용한다.  
> 만약 다른 분석기를 사용하고 싶은 경우 `search_analyzer`를 설정해서 검색시 사용할 분석기를 별도로 지정할 수 있다.  

__similarity__  
> 유사도 측정 알고리즘을 지정한다.  
> 유사도 측정 방식을 기본 알고리즘인 BM25에서 다른 알고리즘을 변경할 수 있다.  

알고리즘 | 설명
----|----
BM25 | Okapi BM25 알고리즘이다. es의 기본 유사도 측정 알고리즘이다.
classic | TF/IDF 알고리즘이다. 문서 내 용어의 개수와 전체 용어의 개수를 이용해 유사도를 계산한다.
boolean | 복잡한 수학적 모델을 사용하지 않고 단순히 boolean 연산으로 유사도를 측정한다. <br/>score는 검색어 일치 여부에 따라 결정되며, 검색 결과의 일치 여부에 따라 쿼리의 가중치(boost)에 사용된 점수로만 유사도를 계산 한다.  

__store__  
> 필드의 값을 저장해 검색 결과에 값을 포함하기 위한 매핑 파라미터다. 기본적으로 ES는 _source에 색인된 문서가 저장된다.  
> 하지만 store매핑 파라미터를 사용하면 해당 필드를 자체적으로 저장할 수 있다.  
> 예를 들어 10개의 필드가 존재하고 해당 필드에 데이터를 매핑한 상태라면   
> _source를 로드해서 해당 필드를 찾는 것 보다 사용할 필드만 로드해서 사용하는 편이 효율적이다.  
> 하지만 해당 매핑 파라미터를 사용하면 디스크를 더 많이 사용한다.  
* 문서가 아닌 필드를 저장하여 빠른 검색을 하기 위한?  

__term_vector__
> 분석된 용어의 정보를 포함할지 결정하는 파라미터.
> 설정 가능한 인자는 다음과 같다.  

인자 | 설명
----|----
no | 텀벡터를 저장하지 않는다.
yes | 필드와 용어만 저장한다.
with_positions | 용어, 용어의 시작과 끝 위치를 저장한다.
with_offsets | 용어, 문자 오프셋을 저장한다.
with_positions_offsets | 용어, 용어의 시작과 끝 위치, 문자 오프셋을 모두 저장한다.

* 참고 : https://zeffortal.blog.me/30071840383  
<br/>  

## 3.2 메타필드
메타 필드는 ES에서 생성한 문서에서 제공하는 특별한 필드다.  
메타데이터를 저장하는 특수 목적의 필드로서 이를 이용하면 검색 시 문서를 다양한 형태로 제어하는 것이 가능하다.  
<br>
색인된 문서가 다음과 같다고 할 때.

```json
{
  "_index": "movie_search",
  "ᅳtype": "_doc",
  "_id": "8",
  "_score": 1,
  "_source": {
    "movieCd": "20178401",
    "movieNm": "검객",
    "movieNmEn": "",
    "prdtYear": "2017",
    "openDt": "",
    "typeNm": "장편",
    "prdtStatNm": "후반작업",
    "nationAlt": "한국",
    "genreAlt": "사극, 액션"
  }
}
```

`_source`를 제외한 나머지가 메타필드이다!!  

### 3.2.1 _index
`_index`는 문서가 속한 인덱스 이름이다. 인덱스 정보를 확인할 때 쓸 수 있다.  
집계 API를 사용 해 보면 인덱스 정보를 확인 할 수 있다.  
```json
POST movie_search/_search
{
  "size": 0,
  "aggs": {
    "indices": {
      "terms": {
        "field": "_index",
        "size": 10
      }
    }
  }
}
```
* 직접 실행 해 보세요.

### 3.2.2 _type
문서의 매핑 타입 정보이다. 인덱스 내부에서 타입별로 몇개의 문서가 있는지 확인 할 수 있다.  

### 3.2.3 _id
문서를 식별하는 유일한 키 값이다. 한 인덱스에서 색인된 문서마다 다른 키값을 가진다.

### 3.2.4 _uid
특수한 목적의 식별키다. "#" 태그를 사용해 _type과 _id값을 조합해 사용한다.  
하지만 내부적으로만 사용되기 때문에 검색 시 조회되는 값은 아니다.

### 3.2.5 _source
문서의 원본 데이터를 제공한다. 내부에는 색인 시 전달된 원본 JSON 문서가 저장되어 있다.  
일반적으로 원본 JSON 문서는 검색결과로 표시할 때 사용한다.  
<br>
`_reindex` API나 스크립트를 사용해 해당 값을 계산할 때 활용 할 수 있다.  
예를 들어 movie_search 인덱스의 movieCd = "20173732" 인 값만 조회해서 prdtYear값을 변경하여 재색인을 해보자.

1. 재색인을 위한 reindex_movie 인덱스를 생성
```
PUT /reindex_movie
```

2. reindex API를 사용하여 재색인 한다. 이때 prdtYear를 +1하기 위해 필드 접근표기법(ctx._source.prdtYear)을 사용한다.
```json
POST /_reindex
{
    "source" : {
        "index" : "movie_search",
        "query" : {
            "match" : {
                "movieCd" : "20173732"
            }
        }
    },
    "dest" : {
        "index" : "reindex_movie"
    }, 
    "script" : {
        "source" : "ctx._source.prdtYear++"
    }
}
```

### 3.2.6 _all
색인에 사용된 모든 필드 정보를 가진 메타필드.  
모든 필드의 내용이 하나의 텍스트로 합쳐져서 제공된다.  
특정 필드가 아닌 문서 전체 필드에서 특정 키워드를 검색한다면 _all 을 사용하면 된다.  

예를들어 다음과 같은 문서가 색인되었을 때  
```json
PUT movie_index_meta_fields_all/_doc/20173732
{
    "movieCd" : "20173732", 
    "movieNm":"살아남은 아이", 
    "movieNmEn":"Last Child", 
    "prdtYear" :"2017"  
}
```
_all 메타 필드에는 "20173732 살아남은 아이 Last Child 2017" 이 생성되어 저장된다.  
그래서 이를 이용하면 통합 검색을 구현할 때 유리할 수 있다.
그러나... 데이터 크기가 커서 es 6.0 이상 부터는 폐기되었다.. 
es 6.0 이상 부터는 copy_to를 사용하면 된다.

### 3.2.7 _routing
특정 문서를 특정 샤드에 저장하기 위해 사용한다.  
기본적으로 색인을 하면 문서는 다음 수식에 따라 문서id를 이용해 색인될 샤드를 결정한다.  
별도의 설정없이 문서를 색인하면 샤드에 골고루 분산되어 저장된다.
```
Hash (document_id) % num_of_shards
```
특정 문서를 하나의 샤드에 저장하고 싶을 때, _routing을 사용한다. 다음과 같이 샤드가 결정된다.
```
Hash (_routing) % num_of_shards
```

_routing을 사용해 색인한 경우,  검색할때도 _routing 값을 지정해야 한다.  

1. 색인
```Json
PUT movie_routing/_doc/1?routing=ko 
{
    "repGenreNm": "한국어",
    "movieNm": "살아남은 아이"
}
```

2. 검색
```
POST movie_routing/_doc/_search?routing=ko
```
3. 결과
```
{
  "hits": {
    "total": 3,
    "max_score": 1,
    "hits": [
      {
        "_index": "movie_routing",
        "_type": "doc",
        "_id": "1",
        "_score": 1,
        "_routing": "ko",
        "_source": {
          "repGenreNm": "한국어",
          "movieNm": "살아남은 아이"
        }
      }
    ]
  }
}
```

## 3.3 필드 데이터 타입
필드에는 다음과 같은 데이터 타입을 지정 할 수 있다.  
### 3.3.1 Keyword
* 키워드 형태로 사용할 데이터에 적합한 타입.  
* `별도의 분석기를 거치지 않고` 원문 그대로 색인하기 때문에 특정코드나 키워드 등 정형화된 콘텐츠에 주로 사용. 
    * 형태소 분석기를 사용하지 않음
    * 검색 시 필터링 되는 항목
    * 정렬이 필요한 항목
    * 집계해야 하는 항목
* 카테고리명, 브랜드명, 셀러명, 코드 등...

### 3.3.2 Text
* 색인시 지정된 분석기가 필드의 데이터를 문자열로 인식하고 분석한다.
    * 기본은 Standard Analyzer
* 전문 검색이 가능하다는 점이 가장 큰 특징
    * 전체 텍스트가 토큰화 되어 특정 단어를 검색하는 것이 가능
* 만약 정렬이나 집계, 필터링 해야 할 경우 (카테고리명, 브랜드명 등..) keyword 타입을 동시에 갖도록 멀티 필드로 설정
```json
PUT movie_search/_mapping/_doc
{
    "properites" :{
        "moveComment" : {
            "type" : "text",
            "fields" : {
                "movieComment_keyword" : {
                    "type" : "keyword"
                }
            }
        }
    }
}
```
* 주요 속성
    * index : 해당 필드를 검색에 사용할지 설정, 기본값 true
    * norms : 유사도 점수를 산정할 때 필드 길이를 고려할지 결정. 기본값 true
    * store : 필드값을 필드와 별도로 _source에 저장하고 검새가 가능하게 할지 설정. 기본값 false

### 3.3.3 Array
* 문자열이나 숫자처럼 일반적인 값을 지정할 수도 있지만 객체 형태로도 정의할 수 있다.
* `Array 타입에 저장되는 값은 모두 같은 타입으로만 구생해야 한다.`
```json
PUT movie_search/_doc/6
{
    "title" : "해리포터와 마법사의 돌"
    "companies" : [
        {
            "companyName" : "워너브라더스"
        }, 
        {
            "companyName" : "Heyday Films"
        }
    ]
}
```
* 데이터가 배열로 저장되면 한 필드내의 검색은 기본적으로 `OR` 조건 검색이 된다.  


### 3.3.4 Numeric

### 3.3.5 Date
* Date 타입은 JSON포맷에서 문자열로 처리
    * 기본형 "yyyy-MM-ddTHH:mm:ssZ"
* 크게 세가지 형태를 제공한다. 모든 형태는 내부적을 UTC의 밀리초 단위로 변환해 저장한다.
    * 문자열이 포함된 날짜 : "2019-07-01 00:00:00", "2019/07/01 00:00:00"
    * ISO_INSTANT 포맷 : "2019-07-01T00:00:00Z"
    * 밀리초 : UTC 기준으로 1970년 1월 1일 0시 0분 0초부터 현재까지 경과된 밀리초

### 3.3.6 Range
* 범위가 있는 데이터를 저장할 때 사용.

타입 | 설명
---|---
integer_range | 부호있는 32비트 정수 범위
float_rage | 부호있는 32비트 실수 범위
long_range | 부호있는 64비트 정수 범위
double_range | 부호있는 64비트 실수 범위
date_range | 64비트 정수 형태의 밀리초로 표시되는 날짜값의 범위
ip_range | IPv4, IPv6 주소를 지원하는 IP값

```json
PUT movie_search/_mapping/_doc
{
    "properites" :{
        "showRange" : {
            "type" : "date_range"
        }
    }
}

PUT movie_search/_doc/2
{
    "showRange" : {
        "gte" : "2001-01-01",
        "lte" : "2001-12-13"
    }
}
```
* 판매기간, 전시기간 등에 쓸 수 있으려나..

### 3.3.7 Boolean
* true, flase 를 문자열로 젖아

### 3.3.8 Geo-Point
* 위도, 경도 등 위치 정보 데이터를 저장할 때 사용.
* 위키 기반 쿼리를 이용해 반경 내 쿼리, 위치별 정렬 등을 사용할 수 있다.

```json
PUT movie_search/_mapping/_doc
{
    "properites" :{
        "fileLocation" : {
            "type" : "geo_point"
        }
    }
}

PUT movie_search/_doc/3
{
    "title" : "해리포터와 마법사의 돌"
    "fileLocation" : {
        "lat" : 55.4155828,
        "lon" : -1.7081091
    }
}
```

### 3.3.9 IP

### 3.3.10 Object
```json
PUT movie_search/_mapping/_doc
{
    "properites" :{
        "companies" : {
            "properties" : {
                "companyName" : {
                    "type" : "text"
                }
            }
        }
    }
}

PUT movie_search/_doc/5
{
    "title" : "해리포터와 마법사의 돌",
    "companies" : {
        "companyName" : "워너브라더스"
    }
}
```

### 3.3.11 Nested
* Object 객체 배열을 독립적으로 색인하고 질의하는 형태의 데이터 타입
* 일반적인 배열은 OR 검색이므로 배열 중 정확하게 일치하는 객제 데이터를 찾기 위해 사용.
* 만약 다음과 같이 색인이 되었다면 companyCd = "2" && companyNme = "워너브라더스" 검색하면 결과가 나온다.
```json
PUT movie_search/_doc/6
{
    "title" : "해리포터와 마법사의 돌",
    "companies" : [
        {
            "companyCd" : "1", 
            "companyName" : "워너브라더스"
        }, 
        {
            "companyCd" : "2", 
            "companyName" : "Heyday Films"
        }
    ]
}
```
* 두 조건이 일치하는 결과만 검색하기 위해 Nested 타입을 쓴다.

```json
PUT movie_search/_mapping/_doc
{
    "properites" :{
        "companies_nested" : {
            "type" : "nested"
        }
    }
}

PUT movie_search/_doc/8
{
    "title" : "해리포터와 마법사의 돌",
    "companies_nested" : [
        {
            "companyCd" : "1", 
            "companyName" : "워너브라더스"
        }, 
        {
            "companyCd" : "2", 
            "companyName" : "Heyday Films"
        }
    ]
}

POST movie_search/_search
{
    "query" : {
        "nested" : {
            "path" : "companies_nested",
            "query" : {
                "bool" {
                    "must" : [
                        {
                            "match" : {
                                "companies_nested.companyName" : "워너브라더스"
                            }
                        },
                        {
                            "match" : {
                                "companies_nested.companyCd" : "2"
                            }                            
                        }
                    ]
                }
            }
        }
    }
}
```
<br/>  

## 3.4 엘라스틱서치 분석기
### 3.4.1 텍스트 분석 개요
* ES는 문서를 색인하기 전에 문서의 필드 타입을 확인하고 text타입이면 분석기를 사용한다.
* 텍스트가 분석 되면 개별 텀으로 나뉘어 형태소 형태로 분석된다.

```json
{
	"analyzer" : "standard",
	"text" : "우리나라가 좋은 나라, 대한민국 화이팅"
}

{
    "tokens": [
        {
            "token": "우리나라가",
            "start_offset": 0,
            "end_offset": 5,
            "type": "<HANGUL>",
            "position": 0
        },
        {
            "token": "좋은",
            "start_offset": 6,
            "end_offset": 8,
            "type": "<HANGUL>",
            "position": 1
        },
        {
            "token": "나라",
            "start_offset": 9,
            "end_offset": 11,
            "type": "<HANGUL>",
            "position": 2
        },
        {
            "token": "대한민국",
            "start_offset": 13,
            "end_offset": 17,
            "type": "<HANGUL>",
            "position": 3
        },
        {
            "token": "화이팅",
            "start_offset": 18,
            "end_offset": 21,
            "type": "<HANGUL>",
            "position": 4
        }
    ]
}
```

### 3.4.2 역색인 구조
* 간단히 정리하면 
    * 모든 문서가 가지는 단어의 고유 단어 목록
    * 해당 단어가 어떤 문서에 속해 있는지에 대한 정보
    * 전체 문서에 각 단어가 몇 개 들어있는지에 대한 정보
    * 하나의 문서에 단어가 몇 번씩 출현했는지에 대한 빈도
* 색인할 때 특정한 규칙과 흐름에 의해 텍스트를 변경하는 과정을 분석(Analyze)이라 하고, 해당 처리는 분석기(Analyzer) 라는 모듈을 조합해서 이뤄진다.

### 3.4.3 분석기의 구조
분석기는 다음과 같은 프로세스로 동작한다.

1. __CHARCTER FILTER__
    * 문장을 특정한 규칙에 의해 수정한다.
    * 문장을 분석하기 전에 텍스트의 특정 단어를 변경하거나, HTML 태그를 제거하는 역할을 한다.
    * 텍스트를 개별 토큰화 하기 전의 전처리 과정이며, ReplaceAll() 처럼 패턴으로 텍스트를 변경하거나 사용자가 정의한 필터를 적용 할 수 있다.
1. __TOKENIZER FILTER__
    * 수정한 문장을 개별 토큰으로 분리한다.
    * 분석기를 구성할 때 하나만 사용할 수 있으며 텍스트를 어떻게 나눌 것인지를 정의한다.
1. __TOKEN FILTER__
    * 개별 토큰을 특정한 규칙에 의해 변경한다.
    * 토근화된 단어를 하나씩 필터링해서 사용자가 원하는 토큰으로 변환한다.
    * 예를 들어, 불필요한 단어를 제거하거나 ㄹ동의어 사전을 만들어 단어를 추가하거나, 영문 단어를 소문자로 변환하는 등의 작업을 실행할 수 있다.
    * Token Filter는 여러 단계가 순차적으로 이뤄지며 순서를 어떻게 지정하느냐에 따라 결과가 달라 질 수 있다.

```
[문장] 
-> charater Filter -> [가공된 문장] 
-> Tokenizer Filter -> [Terms] 
-> Token Filter with 사전 -> [변경된 Terms] 
-> index
```

분석기를 정의해 보자.
```json
PUT /movie_analyzer
{
	"settings" : {
		"index" : {
			"number_of_shards" : 5,
			"number_of_replicas" : 1 
		}, 
        "analysis" : {
            "analyzer" : {
                "custom_movie_analyzer" : {
                    "type" : "custom",
                    "char_filter" : [
                        "html_strip"
                    ],
                    "tokenizer" : "standard",
                    "filter" : [
                        "lowercase"
                    ]
                }
            }
        }
	}
}
```
* [char_filter] html_strip
    * 전체 문장에서 HTML 태그를 제거한다.
* [tokenizer] standard
    * 특수문자 혹은 공백을 기준으로 Term을 분할한다.
* [filter] lowercase
    * 모든 토큰을 소문자로 변환한다.

```
<b>Elasticsearch</b> is cool  
    |
    | Charater Fiter : html_strip -> html 제거  
    V
Elasticsearch is cool  
    |
    | Tokenizer Filter : standard -> 특수문자 혹은 공백 기준으로 Term 분리  
    V
Elasticsearch,is,cool  
    |
    | Token Filter : lowercase 
    V
elasticsearch,is,cool  
```

#### 3.4.3.1 분석기 사용법
1. 분석기를 이용한 분석
```json
POST _analyze
{
	"analyzer" : "standard",
	"text" : "캐리비안의 해적"
}
```

1. 필드를 이용한 분석 -> 요건 좀 더 확인 필요
```json
POST movie_analyzer/_analyze
{
    "field" : "title",
    "text" : "캐리비안의 해적"
}
```

1. 색인과 검색 시 분석기를 각각 설정
```json
PUT /movie_analyzer
{
    "settings": {
        "analysis": {
            "analyzer": {
                "movie_lower_test_analyzer": {
                    "type" : "custom",
                    "tokenizer": "standard",
                    "filter" : [
                        "lowercase"
                    ]
                },
                "movie_stop_test_analyzer" : {
                    "type" : "custom",
                    "tokenizer" : "standard",
                    "filter" : [
                        "lowercase",
                        "english_stop"
                    ]
                }
            },
            "filter" : {
                "english_stop" : {
                    "type" : "stop",
                    "stopword" : "_english_"
                }
            }
        }
    },
    "mappings" : {
        "_doc" : {
            "properties" : {
                "title" : {
                    "type" : "text",
                    "analyzer" : "movie_stop_test_analyzer",
                    "search_analyzer" : "movie_lower_test_analyzer"
                }
            }
        }
    }
}
```

#### 3.4.3.2 대표적인 분석기
1. __Standard Analyzer__  
    * 공백 혹은 특수 기호를 기준으로 토큰을 분리하고 모든 문자를 소문자로 변경하는 토큰 필터를 사용
    * 구성 요소
        * Standard Tokenizer
        * Standard Token Filter
        * Lower Case Token Filter

        파라미터 | 설명
        ---|---
        max_token_length | 최대 토큰 길이를 초과하는 토큰은 설정된 length 간격으로 분할한다. 기본 255자
        stopwords | 사전 정의된 불용어 사전을 사용. 기본 사용하지 않음
        __stopwords_path__ | 불용어 사전 파일을 사용할 경우 서버의 경로로 설정한다.

1. __Whitespace 분석기__  
    * 공백 문자열을 기준으로 토큰을 분리

1. __Keyword 분석기__  
    * 전체 문자열을 하나의 키워드처럼 처리. 토큰화 작업을 하지 않는다.

### 3.4.4 전처리 필터 (Character Filter)
1. __Html Strip char 필터__  
    * 문장에서 html 을 제거

        파라미터 | 설명
        ---|---
        escaped_tags | String 배열에 정의된 태그만 삭제한다. 기본값은 전체 태그를 삭제한다.

### 3.4.5 Tokenizer Filter
분석기를 구성하는 가장 핵심적인 구성 요소.  
1. __Standard Tokenizer__
1. __Whitespace Tokenizer__
1. __Ngram Tokenizer__
    * 기본적으로 한 글자씩 토큰화 한다.
    * 특정 문자를 지정할 수도 있으며, 이 경우 지정된 문자의 목록중 하나를 만날 때마다 단어를 자른다.
    
        파라미터 | 설명
        ---|---
        min_gram | Ngram을 적용할 문자의 최소 길이. 기본 1
        max_gram | Ngram을 적용할 문자의 최대 길이. 기본 2
        token_chars | 토큰에 포함할 문자열을 지정. 다음과 같은 옵션을 제공한다.<br/>* letter (문자)<br/>* digit (숫자)<br/>* whitespace (공백)<br/>* punctuation (구두점)<br/>* symbol (특수기호)

    * 불용어는 제외한다.
    * 자동완성을 만들 때 유용하게 활용
1. __Edge Ngram Tokenizer__
    * 지정된 문자 목록 중 하나를 만날 때마다 시작 부분을 고정시며 단어를 자르는 방식.
    * 자동완성을 만들 때 유용하게 활용

    ```json
    PUT movie_engram_analyzer
    {
        "settings" : {
            "analysis" : {
                "analyzer" : {
                    "edge_ngram_analyzer" : {
                        "tokenizer" : "edge_ngram_toknizer"
                    }
                }, 
                "tokenizer" : {
                    "edge_ngram_toknizer" : {
                        "type" : "edge_ngram",
                        "min_gram" : 2,
                        "max_gram" : 10,
                        "toke_chars" : [
                            "letter"
                        ]
                    }
                }
            }
        }
    }

    POST movie_engram_analyzer/_analyze
    {
        "tokenizer" : "edge_ngram_toknizer",
        "text" : "Harry Portter and the Chamber of Secrets"
    }
    ```
1. __Keyword Tokenizer__
    * 텍스트를 하나의 토큰으로 만든다.

        파라미터 | 설명
        ---|---
        buffer_size | term을 버퍼로 읽을 문자수를 지정. 기본 256

### 3.4.6 Token Filter
토크나이저에서 분리된 토큰들을 변형하거나 추가, 삭제할 때 사용하는 필터.
토크나이저에 의해 토큰이 모두 분리된 후에 동작하기 때문에 독립적으로 사용할 수 없다.

1. __Ascii Folding Token Filter__
    * ASCII 코드에 해당하는 127개의 알파벳, 숫자, 기호에 해당하지 않는 문자를 ASCII 요소로 변경한다.

1. __Lowercase Token Filter__
    * 전체 문자열을 소문자로 변환

1. __Uppercase Token Filter__
    * 전체 문자열을 대문자로 변환

1. __Stop Token Filter__
    * 불용어를 처리 할 수 있는 필터
    * 인덱스로 만들고 싶지 않거나 검색되지 않게 하고 싶은 단어를 등록하여 불용어 사전을 만든다.

        파라미터 | 설명
        ---|---
        stopwords | 불용어를 직접 등록하여 사용
        stopwords_path | 불용어 사전의 경로를 지정. 해당 경로는 es 설치 폴더의 config 폴더안에 생성한다.
        ignore_case | true로 지정할 경우 모든 단어를 소문자로 변경해서 저장한다. 기본 false

1. Stemmer Token Filter
    * Stemming 알고리즘을 사용

1. __Synonym Token Filter__
    * 동의어를 처리 할 수 있는 필터  

        파라미터 | 설명
        ---|---
        synonyms | 동의어로 사용할 단어를 등록한다.
        synonyms_path | 동의어 사전의 경로를 지정. 해당 경로는 es 설치 폴더의 config 폴더안에 생성한다.

1. __Trim Token Filter__
    * 앞뒤 공백을 제거하는 필터

### 3.4.7 동의어 사전
* __동의어를 추가하는 방법__
    1. 동의어를 매핑 설정 정보에 미리 파라미터로 등록하는 방식
    1. 특정 파일을 별도로 생성해서 관리하는 방식
    * 동의어 사전은 es 설치 경로의 config 디렉토리에 생성해야 한다.
        ```
            {es 설치 경로}/config/analysis/synonyms.txt 이고
            설정은 "synonyms_path" : "analysis/synonyms.txt"
        ```

* __동의어 추가__
    * 동의어 추가, 치환 두가지 방법으로 등록 할 수 있다.
        ```
        Elasticsearch,엘라스틱서치 // 동의어 추가
        Harry => 해리 // 동의어 치환
        ```
    * 구 버전에서는 Synonym 토큰 필터 전에 lowercase 혹은 uppercase가 적용 되었다면 사전에는 변경된 단어를 등록해야 했다.
        * 최신 버전에서는 대/소문자를 구분하지 않는다.

    * 동의어 사전은 실시간으로 적용되지 않는다.
        * 인덱스를 Reolad 해야 반영된다.
    * 동의어 사전은 색인과 검색 시점에 사용된다.
        * 검색은 사전 변경시 즉시 반영된다.  
  
<br>
## 3.5 Document API 이해하기

### 3.5.1 문서 파라미터
* 버전 관리
    * 색인된 모든 문서는 버전값을 가지고 있다.
    * 버전 정보는 색인할 때결과에 포함된다.
    * 최초 1을 갖게 되고 변경시 버전값이 증가한다.
    * Update API 는 내부적으로 스냅샷을 생성해서 문서를 수정하고, 인덱스에 재색인하는데. 이때 버전 정보를 사용한다.
        * 스냅샷이 생성된 사이에 버전값이 달라졌다면 실패로 처리한다.  

* 오퍼레이션 타입
    * 색인시 ID가 존재하면 Update, 없을 경우 Create가 실행된다.
    * 다음과 같이 op_type파라미터를 사용하면 무조건 create가 실행되고 에러가 발생한다.
        ```
        PUT movie_dynamic/_doc/1?op_type=create
        ```  

* 타임아웃 설정
    * 색인 요청시 대부분 즉시 처리되지만 작업이 지연되면 기본 1분 후 요청 자체가 실패한다.
    * 1분 이상 작업해야 할 경우 timeout을 설정하여 처리 할 수 있다.
        ```
        PUT movie_dynamic/_doc/1?timeout=5m
        ````  

* 인덱스 매핑 정보 자동 생성
    * Index API 로 문서를 색인할 때 기 정의되지 않은 필드가 존재할 경우, 기본적으로 동적 매핑이 허용되어 새로운 필드가 생성된다.
        * 의도하지 않은 오류가 발생할 수 있다.
    * elasticsearch.yml 에 다음을 설정하면 동적 매핑을 막을 수 있다.
        ```
        action.auto_create_index //인덱스의 자동 생성여부를 설정. 기본 true
        index.mapper.dynamic // 동적 매핑 사용 여부를 설정. 기본 true
        ```

### 3.5.2 Index  API

```json
PUT movie_dynamic/_doc/1
{
    "movieCd" : "20173732",
    "movieNm" : "살아남은 아이"
}

// result
{
    "_index" : "movie_dynamic",
    "_type" : "doc",
    "_id" : "1",
    "_version" : 1,
    "result" : "created",
    "_shards" : {
        "total" : 2,
        "successful" : 1,
        "failed" : 0
    },
    "_seq_no" : 0,
    "_primary_term" : 1
}
```

* _shard 중 total 은 복제 되어야 할 전체 샤드수, successful은 성공한 샤드수, failed는 실패한 샤드수이다.
* successful 이 0일 경우 실패로 간주한다.  

### 3.5.3 Get API
* 특정 문서를 조회 할 경우 사용.
* 특정 필드를 제외하고 조회 할 경우 다음과 같이 사용.
    ```
    GET movie_dynamic/_doc/1?_source_exclude=movieNm
    ```  

### 3.5.4 Delete API
* Delete API를 사용하면 result 항목에 "deleted" 가 반환되며 _version 이 1 증가하는 것을 확인 할 수 있다.  

### 3.5.5 Delete By Query API
* 질의 결과에 대한 문서만 삭제할 경우 사용.
```json
POST movie_dynamic/_delete_by_query
{
    "query" : {
        "term" : {
            "movieCd" : "20173732"
        }
    }
}

// 결과
{
    "took" : 17,
    "timed_out" : false,
    "total" : 1,
    "deleted" : 1,
    "batches" : 1,
    "version_conflicts" : 0,
    "noops" : 0,
    "retries" : {
        "bulk" : 0,
        "search" : 0
    },
    "throttled_millis" : 0,
    "requests_per_second" : -1.0,
    "throttled_until_millis" : 0,
    "failures" "[]
}
```
* Delete By Query API 실행시 해당 인덱스의 스냅샷의 문서 버전을 기반으로 삭제를 실행한다.
* 만일 대량 수정 작업 중에 삭제를 실행하면 버전이 일치하지 않아 실패한다.
    * version_conflicts에 삭제에 실패한 문서의 건수를 출력한다.  

### 3.5.6 Update API 
* Update API를 사용하면 스크립트를 바탕으로 문서를 수정할 수 있다.
    * "ctx._source.필드명" 으로 필드에 접근 할 수 있다.
        ```json
        POST movie_dynamic/_doc/1/_update
        {
            "script" : {
                "source" : "ctx._source.counter += params.count",
                "lang" : "painless",
                "params" : {
                    "count" : 1
                }
            }
        }
        ```
* ES의 Update는 실제 update가 아니다.
* Update API가 호출되면 index에서 문서를 가져와 스크립트를 실행한 후, 재색인을 실행한다.
    * 이런 이유로 _source 필드가 활성화 되어 있어야 한다.
    * 또한 ctx필드는 _source 변수 뿐 아니라 _index, _type, _id, _version, _routing, _now 등 추가적인 변수도 사용할 수 있다.  

### 3.5.7 Bulk API
* Get / Delete / Update 는 하나의 문서만을 대상으로 동작한다.
* Bulk API는 한번의 호출로 다수의 문서를 색인하거나 삭제 할 수 있다.
    * 특히 색인 작업은 한번에 처리하여 색인 속도를 크게 향상 시킬 수 있다.  

```json
POST _bulk
{"index" : {"_index" : "movie_dynamic_bulk", "_type" : "_doc", "_id" : "1"}}
{"title" : "살아 남은 아이"}
{"index" : {"_index" : "movie_dynamic_bulk", "_type" : "_doc", "_id" : "2"}}
{"title" : "해리포터와 비밀의 방"}
{"index" : {"_index" : "movie_dynamic_bulk", "_type" : "_doc", "_id" : "3"}}
{"title" : "어벤저스"}
{"update" : {"_index" : "movie_dynamic_bulk", "_type" : "_doc", "_id" : "1"}} 
{"doc" : {"title" : "살아 남은 아이2"}}
```

* 여러 건의 데이터가 한 번에 처리 되기 때문에, 도중에 실패가 발생하더라도 이미 갱신되거나 수정된 결과는 롤백되지 않는다.  

### 3.5.8 Reindex API
* 한 인덱스에서 다른 인덱스로 문서를 복사할 때 일반적으로 사용한다.
    ```json
    POST /_reindex
    {
        "source" : {
            "index" : "movie_dynamic"
        }, 
        "dest" : {
            "index" : "movie_dynamic_new"   
        }
    }
    ```

* 조건을 걸어 복사 하고 싶을 때는 쿼리를 포함한다.
    ```json
    POST /_reindex
    {
        "source" : {
            "index" : "movie_dynamic",
            "type" : "_doc",
            "query" : {
                "temr" : {
                    "title.keyword" : "프렌즈: 몬스터섬의비밀"
                }
            }
        }, 
        "dest" : {
            "index" : "movie_dynamic_new"
        }
    }
    ```

* 정렬까지 적용해 보자.
    ```json
    POST /_reindex
    {
        "size" :10000,
        "source" : {
            "index" : "movie_dynamic",
            "type" : "_doc",
            "query" : {
                "temr" : {
                    "title.keyword" : "프렌즈: 몬스터섬의비밀"
                }
            },
            "sort" : {
                "counter" : "desc"
            }
        }, 
        "dest" : {
            "index" : "movie_dynamic_new"
        }
    }
    ```  

    * 기본적으로 Reindex API는 1,000건 단위로 스크롤을 실행한다.
    * 이때 size 를 지정해 스크롤 크기를 변경 할 수 있다.
    * 많은 양의 ㅁ누서를 복사해야 한다면 size 항목을 늘려서 전체적인 속도 향상을 꾀할 수 있다.  

<br>
# 4. 데이터 검색 
## 4.1 검색 API
문장은 색인 시점에 텀으로 분해되고, 검색 시에 이 텀을 일치시켜야 검색이 가능해 진다.  
인덱스를 삭제하고 재 생성 해 보자.

```
curl -XPOST 'http://localhost:9200/_snapshot/javacafe/movie-search/_restore'
```  

생성된 인덱스는 다음과 같이 확인해 볼 수 있다.
```json
GET /_cat/indices/movie_search?v&pretty
```  

### 4.1.1 검색 질의 표현 방식
es의 검색 API는 기본적으로 질의 (Query)를 기반으로 동작한다.  

* URI 검색 루씬에서 사용하던 전통적인 방식
    * GET method를 사용
    * 파라미터를 "key=value" 형태로 전달
    ```
    GET /movie_search/_search?q=prdtYear:2018
    ```
      
* Request Body 검색 : RESTful API를 사용
    * JSON 형태의 표현을 효율적으로 사용하기 위해 Query DSL 문법을 사용한다.
        * Query DSL 참고 : http://www.querydsl.com/static/querydsl/3.4.3/reference/ko-KR/html_single/
    * 위 URI는 다음과 같이 표현된다ㅣ.
    ```json
    POST /movie_search/_search
    {
        "query" : {
            "term" : {
                "prdtYear" : "2018"
            }
        }
    }
    ```

### 4.1.2 URI 검색
사용 가능한 파라미터는 다음과 같다.  

파라미터 | 기본값 | 설명
---|---|---
q | - | 검색을 수행할 쿼리 문자열 조건을 지정
df | - | 쿼리에 검색을 수행할 필드가 지정되지 않았을 경우<br/>기본값으로 검색할 필드를 지정한다.
analyzer | 검색 대상 필드에 설정된 형태소 분석기 | 쿼리 문자열을 형태소 분석할 때 사용할 형태소 분석기를 지정한다.
analyzer_wildcard | false | 접두어/와일드카드(*) 검색 활성화 여부를 지정한다.
default_operator | OR | 두 개 이상의 검색 조건이 쿼리 문자열에 포함된 경우 검색 조건 연산자를 설정한다.
_source | true | 검색 결과에 문서 본문 포함 여부를 지정한다.
sort | - | 검색 결과의 정렬 기준 필드를 지정
from | - | 검색을 시작할 문서의 위치를 설정
size | - | 반환할 검색 결과 개수를 설정

* q파라미터는 '\[필드명]:검색어' 형태로 입력 할 수 있으며, 다음과 같이 여러개의 필드를 검색 할 경우 공백으로 구분한다.

```
POST /movie_search/_search?q=movieNmEn:* AND prdtYear:2017&....
```

### 4.1.3 Request Body 검색
* 이건 늘 했던 거라...  

<br/>
## 4.2 Query DSL 이해하기
### 4.2.1 Query DSL 쿼리의 구조
* 기본적인 요청을 위한 JSON 구조는 다음과 같다.  
    ```json
    {
        "size" : ,      // 기본값은 10
        "from" : ,      // 기본값은 0
        "timeout" : ,   // 기본값은 무한대
        "_source" : {}, // 
        "query" : {},   // 조건문을 정의하는 공간
        "aggs" : {},    // 통계 및 집계 데이터를 정의할 때 사용하는 공간
        "sort" : {}     // 정렬을 정의하는 공간
    }
    ```  

* 응답 JSON 구조는 다음과 같다.
    ```json
    {
        "took" : ,      // 쿼리를 실행한 시간을 나타낸다.
        "timed_out" : , // timeout을 초과할 경우를 나타낸다.
        "_shards" : {
            "total" :,          // 쿼리를 요청한 전체 샤드의 개수를 나타낸다.
            "successful" : ,    // 쿼리 요청에 성공적으로 응답한 샤드의 갯수를 나타낸다.
            "failed" :          // 검색 요청에 실패한 샤드의 개수를 나타낸다.
        },
        "hits" : {
            "total" : ,         // 검색어에 매칭된 문서의 전체 개수를 나타낸다.
            "max_score" : ,     // 일치하는 문서의 스코어 값 중 가장 높은 값을 출력한다.
            "hits" : []         // 각 문서 정보와 스코어 값을 보여준다.
        }
    }
    ``` 


### 4.2.2 Query DSL 쿼리와 필터

분석기에 의한 전문 분석이 필요한 경우 (Query) 와 단순히 "yes/no" 로 판단 할 수 있는 조건 검색(Filter)을 구분한다.
* 쿼리 컨텍스트
    * 문서가 쿼리와 얼마나 유사한지르 ㄹ스코어로 계산한다.
    * 질의가 요청될 때마다 엘라스틱서치에서 내부의 루씬을 이용해 계산을 수행한다.
        * 이때 결과가 캐싱되지 않는다.
    * 일반적으로 전문 검색에 많이 사용한다.
    * 캐싱되지 않고 디스크 연산을 수행하기 때문에 상대적으로 느리다.
    * 예시
        ```json
        POST movie_search/_search
        {
            "query" : {
                "match" : {
                    "movieMm" : "기묘한 가족"
                }
            }
        }
        ```  

* 필터 컨텍스트
    * 쿼리의 조건과 문서가 일치하는지 (yes / no)를 구분한다.
    * 별도로 스코어를 계산하지 않고 단순 매칭 여부를 검사한다.
    * 자주 사용되는 필터의 결과는 es가 내부적으로 캐싱한다.
    * 기본적으로 메모리 연산을 수행하기 때문에 상대적으로 빠르다.
    * 예시
        ```json
        POST movie_search/_search
        {
            "query" :{
                "bool" :{
                    "must" : [
                        {
                            "match_all" : {}
                        }
                    ],
                    "filter" : {
                        "term" : {
                            "repGenreNm" : "다큐멘터리"
                        }
                    }
                }
            }
        }
        ```  

### 4.2.3 Query DSL의 주요 파라미터
* __Multi Index 검색__
    * es는 Multi Index 및 Multi Type 검색이 가능하다.
        ```json
        POST movie_search,movie_auto/_search
        {
            "query" : {
                "term" : {
                    "repGenreNm" : "다큐멘터리"
                }
            }
        }
        ```
    * 인덱스 이름을 와일드카드'*' 을 사용하여 검색 가능하다.
        ```
        POST /log-2019-*/_search
        ```  

* 쿼리 결과 페이징
    * `from` 과 `size`를 사용
        ```json
        # 첫 번쩨 페이지 요청
        POST movie_search/_search
        {
            "from" : 0,
            "size" : 5,
            "query" : {
                "term" : {
                    "repNationNm" : "한국"
                }
            }
        }

        # 두 번째 페이지 요청
        POST movie_search/_search
        {
            "from" : 5,
            "size" : 5,
            "query" : {
                "term" : {
                    "repNationNm" : "한국"
                }
            }
        }    
        ```
    * es는 RDB와 다르게 페이징된 해당 문서만 선택적으로 가져오는 것이 아니라 모든 데이터를 읽게 된다.
        * 즉 페이지 번호가 높아질 수록 쿼리 비용도 높아진다.
            * 코난만의 문제는 아니었네 ㅠㅠ  

* 쿼리 결과 정렬
    * `sort`를 사용
        ```json
        POST movie_search/_search
        {
            "query" : {
                "term" : {
                    "repNationNm" : "한국"
                }
            }, 
            "sort" : {
                "prdtYear" : {
                    "order" : "asc"
                }
            }
        } 
        ```
    * 결과 중 스코어 값이 같은 경우에는 두번째 정렬을 사용 할 수 있다.
        ```json
        POST movie_search/_search
        {
            "query" : {
                "term" : {
                    "repNationNm" : "한국"
                }
            }, 
            "sort" : {
                "prdtYear" : {
                    "order" : "asc"
                },
                "score" : {
                    "order" : "desc"
                }
            }
        } 
        ```  

* ___source 필드 필터링__
    * 결과에 대한 필터링이다. = Partial Response
        ```json
        POST movie_search/_search
        {
            "_source" : [
                "movieNm"
            ], 
            "query" : {
                "term" : {
                    "repNationNm" : "한국"
                }
            }
        }
        ```  

* 범위 검색

    문법 | 연산자
    ---|---
    lt | <
    gt | > 
    lte | <=
    gte | >=

    ```json
    POST movie_search/_search
    {
        "query" : {
            "range" : {
                "prdtYear" : {
                    "gte" : "2016", 
                    "lte" : "2017"
                }
            }
        }
    }
    ```  

* operator 설정
    * es는 검색 시 문장이 들어올 경우 기본적으로 OR 연산을 한다. 
    * AND가 결과의 정확도가 높기 때문에 AND로 변경할 필요가 있다.
    * Query DSL에서는 `operator`를 사용하여 연산자를 명시적으로 지정 할 수 있다.
        ```json
        POST movie_serch/_search
        {
            "query" : {
                "match" : {
                    "movieNm" : {
                        "query" : "자전차왕 엄복동",
                        "operator" : "and"
                    }
                }
            }
        }
        ```  

* minimum_should_match 설정
    * OR 연산을 수행할 경우 검색 결과가 너무 많이 나올 수 있다.
    * 이 경우 텀의 개수가 몇 개 이상 매칭될 때만 검색 결과로 나올수 있도록 `minimum_should_match' 을 설정 할 수 있다.
    * 다음과 같은 경우 OR 이지만 2개 이상 매칭되어야 하기 때문에 AND와 결과가 같아진다.
        ```json
        POST movie_search/_search
        {
            "query" : {
                "match" : {
                    "movieNm" : {
                        "query" : "자전차왕 엄복동",
                        "minimum_should_match" : 2
                    }
                }
            }
        }
        ```  

* fuzziness 설정
    * 단순히 같은 값을 찾는 Match Query를 유사한 값을 찾는 Fuzzy Query로 변경할 떄 사용한다.
    * 오차 범위 (fuzziness) 를 2로 설정한다면, 오차범위가 두 글자 이하인 검색 결과까지 포함한다.
    * 오차범위 값으로 0,1,2,AUTO 를 사용할 수 있다.
        * 알파벳에는 유용하지만 한국어에는 적용하기 어렵다.
    * 다음과 같이 설정하면 'Fli High'의 질의에 'Fly High' 가 검색 된다.
        ```json
        POST movie_search/_search
        {
            "query" : {
                "match" : {
                    "movieNm" : {
                        "query" : "Fli High",
                        "fuzziness" : 2
                    }
                }
            }
        }
        ```  

* __boost 설정__
    * 관련성이 높은 필드나 키워드에 가중치를 더 줄 수 있게 해준다.
    * 한글 영화 제목에 좀 더 가중치를 부여해서 검색 결과를 좀 더 상위로 올리고 싶을 때 사용할 수 있다.
        ```json
        POST movie_search/_search
        {
            "query" : {
                "multi_match" : {
                    "query" : "Fly",
                    "fields" : ["movieNm^3", "movieNmEn"]
                }
            }
        }
        ```
    * 한글 영화 제목과 영어 영화 제목에 대해 검색을 하고, 한글 제목이 일치 한다면 한글 영화 제목에서 계산되는 스코어에 가중치 값으로 3을 곱하게 된다.  

<br/>  
## 4.3  Query DSL의 주요 쿼리
### 4.3.1 Match all Query
* match_all 파라미터를 사용하는 Match all Query 는 Index의 모든 문서를 검색하는 쿼리다.
    ```json
    POST movie_search/_search
    {
        "query" : {
            "match_all" : {}
        }
    }
    ```  
<br/>

### 4.3.2 Match Query (Full Text Query)
* 텍스트, 숫자, 날짜 등이 포함된 문장을 형태소 분석을 통해 텀으로 분리한 후 이 텀들을 이용해 검색 질의를 수행한다.
* 검색어가 분석되어야 할 경우에 사용한다.
* 즉. 가장 일반적이고 기본적인 쿼리라는 거다!!
    ```json
    POST movie_search/_search
    {
        "query" : {
            "match" : {
                "movieNm" : "그대 장미"
            }
        }
    }
    ```
    * "그대", "장미" 2개의 텀으로 분리 후 별도 operator가 정의 되지 않아 OR 검색을 한다.  
<br/>  

### 4.3.3 Multi Match Query
* Match Query와 기본적인 사용 방법은 동일 하나, 단일 필드가 아닌 여러 개의 피드를 대상으로 검색할 수 있다.
    ```json
    POST movie_sarch/_search
    {
        "query" : {
            "multi_match" : {
                "query" : "가족", 
                "fields" : ["movieNm", "movieNmEn"]
            }
        }
    }
    ```  
<br/>  

### 4.3.4 Term Query
* 텍스트 형태의 값을 검색하기 위해 es는 다음 두가지 매핑 유형을 지원한다.
    * Text : 필드에 데이터가 저장되기 전에 데이터가 분석되어 역색인 구조로 저장
    * Keyword : 데이터가 분석되지 않고 그대로 필드에 저장
* Term Query는 별도의 분석을 하지 ㅇ낳고 입력된 텍스트가 존재하는 문서를 찾는다.
    * Match Query(Full Text Query)는 쿼리를 수행하기 전에 먼저 분석기를 통해 텍스트를 분석한 후 검색을 수행한다.
* Term Query는 Keyword 데이터 타입을 대상으로 하기 때문에 일반적으로 숫자, Keyword, 날짜 데이터를 쿼리하는데 사용한다.
    ```json
    POST movie_search/_search
    {
        "query" : {
            "term" : {
                "genreAlt" : "코미디"
            }
        }
    }
    ```
* Term Query는 검색어를 하나의 텀으로 처리하기 때문에, 필드에 텀이 정확히 존재하지 않는 경우 검색이 되지 않는다.
    * 영문의 경우에는 대소문자가 다를 경우 검색되지 않는다.  
<br/>  

### 4.3.5 Bool Query
* es에서 RDB의 AND, OR로 묶은 여러 조건을 Where 절에서 사용하는 것과 유사하게 쓸 수 있다.
    * 하나의 쿼리나 여러개의 쿼리를 조합해서 더 높은 스코어를 가진 쿼리 조건으로 검색할 수 있는데.
    * 이 유형의 쿼리를 Compound Query라 하고, Compound Query를 구현하기 위해 Bool Query를 제공한다.
* Bool Query를 상위에 두고 하위에 다른 Query들을 사용해 복잡한 조건의 쿼리문을 작성 할 수 있다.
* Bool Query는 주어진 쿼리와 논리적으로 일치하는 문서를 복합적으로 검색한다.
    * 여러가지 형태 ( AND / OR / NAND / FILTER ) 를 표현 할 수 있다.
* 예시는 다음과 같다.
    ```json
    {
        "query" : {
            "bool" : {
                "must" : [],
                "must_not" : [],
                "should" : [],
                "filter" : []
            }
        }
    }
    ```

    es 속성 | SQL | 설명
    ---|---|---
    must : \[필드\] | AND 컬럼 = 조건 | 반드시 조건에 만족하는 문서를 검색
    must_not : \[필드\] | AND 컬럼 != 조건 | 조건을 만족하지 않는 문서를 검색
    should : \[필드\] | OR 컬럼 = 조건 | 여러 조건 중 하나 이상을 만족하는 문서를 검색
    filter : \[필드\] | 컬럼 IN (조건) | 조건을 포함하고 있는 문서를 출력한다. <br/> 해당 파라미터를 사용하면 스코어별로 정렬되지는 않는다.

* 예시 : 대표 장르가 "코미디"이고, 제작 국가에 "한국"이 포함되어 있으며, 형화 타입 중 "단편"이 제외된 문서를 검색한다면....
    ```json
    POST movie_search/_search
    {
        "query" : {
            "bool" : {
                "must" : [
                    {
                        "term" : {
                            "repGenreNm" : "코미디"
                        }
                    },
                    { 
                        "match" : {
                            "repNationNm" : "한국"
                        }
                    }
                ], 
                "must_not" : [
                    {
                        "match" : {
                            "typeNm" : "단편"
                        }
                    }
                ]
            }
        }
    }
    ```  
<br/>

### 4.3.6 Query String
* es에 기본적으로 내장된 쿼리 분석기를 사용
    ```json
    POST movie_search/_search
    {
        "query" : {
            "query_string" : {
                "default_field" : "movieNm",
                "query" : "(가정) AND (어린이 날)"
            }
        }
    }
    ```
    * "가정" 과 "어린이날" 이 각각 형태소 분석기를 통해 분석되며, 분석된 텀을 대상으로 AND 조건과 만족하는 문서를 검색한다.
    * 주의 할 점은 `기존 텀 쿼리와 다르게 공백은 연산자로 사용되지 않으며, 입력된 텍스트 그대로 형태로 분석기에 전달된다.`  
<br/>  

### 4.3.7 Prefix Query
* 해당 접두어가 있는 모든 문서를 검색한다.
    ```json
    POST movie_search/_search
    {
        "query" : {
            "prefix" : {
                "movieMm" : "자전차"
            }
        }
    }
    ```  
    * movieNm 필드에서 "자전차" 로 시작되는 문서를 검색한다.
<br/>

### 4.3.8 Exists Query
* 필드에 값이 존재하는 (not null) 문서만 검색 할 경우
    ```json
    POST movie_search/_search
    {
        "query" : {
            "exists" : {
                "field" : "movieNm"
            }
        }
    }
    ```  
<br/>

### 4.3.9 Wildcard Query
* 검색어가 와일드카드와 일치하는 문서를 찾는다.
    * 입력된 검색어는 형태소 분석이 이뤄지지 않는다.
* typeNm 필드에서 "장편"을 찾기 위해 다음과 같이 사용한다.
    ```json
    POST movie_search/_search
    {
        "query" : {
            "wildcard" : {
                "typeNm" : "장?"
            }
        }
    }
    ```  
<br/>

### 4.3.10 Nested Query
* SQL에서 지원하는 `join`과 유사한 기능을 수행한다.
* Nested Query는 Nested 데이터 타입의 필드를 검색할 때 사용한다.
    * Nested 데이터 타입은 문서 내부에 다른 문서가 존재할 때 사용한다.
* `path` 옵션으로 중첩된 필드를 명시하고, `query` 옵션에 Nested 필드 검색에 사용할 쿼리를 입력산다.
* 다음과 같이 Nested 타입의 인덱스가 있다면.
    ```json
    PUT movie_nested
    {
       "settings" : {
           "number_of_replicas" : 1,
           "number_of_shards" : 5
       },
       "mapping" : {
           "_doc" : {
               "properties" : {
                   "repGenreNm" : {
                       "type" : "keyword"
                   }, 
                   "companies" : {
                       "type" : "nested",
                       "properties" : {
                           "companyCd" : { "type" : "keyword" },
                           "companyNm" : { "type" : "keyword" }
                       }
                   }
               }
           }
       }
    }
    ```
* Nested Query를 이용해 Child로 저장된 문서의 필드를 검색할 수 있다.
    ```json
    POST movie_nested/_search
    {
        "query" : {
            "bool" : {
                "must" : [
                    {
                        "term" : {
                            "repGenreNm" : "멜로/로맨스"
                        }
                    },
                    {
                        "nested" : {
                            "path" : "companies",
                            "query" : {
                                "bool" : {
                                    "must" : [
                                        {
                                            "term" : {
                                                "companies.companyCd" : "20173401"
                                            }
                                        }
                                    ]
                                }
                            }
                        }
                    }
                ]
            }
        }
    }
    ```
* 참고로 es는 성능상의 이유로 Parent 문서와 Child 문서를 모두 동일한 샤드에 저장한다.
    * 이러한 방식을 통해 네트워크의 비용을 대폭 줄이는 것이 가능해 진다.
    * 하지만 특정 Parent 문서에 포함된 Child 문서가 비정상적으로 커질 경우 샤드의 크기가 일정하게 분배되지 못하는 문제점이 발생할 수 있다.
<br/>

## 4.4 부가적인 검색 API
### 4.4.1 효율적인 검색을 위한 환경설정
* 동적 분배 방식의 샤드 선택
    * es는 부하 분산과 장애 극복을 위해 레플리카 샤드를 함께 사용한다.
    * 원본과 레플리카 양쪽에서 검색을 수행한다면 중복된 결과가 나올수 있어, es는 동일 데이터를 가지고 있는 샤드 중 하나만 선택한다.
    * 특별한 설정이 없다면 라운드로빈방식으로 샤드를 선택한다.
    * 동적 분배 방식의 알고리즘도 제공
        * 동적 분배는 검색 요청의 응답 시간, 요청을 수행하는 스레드 풀의 크기등을 고려해 최적의 샤드를 결정하는 방색이다.
    * 동적 분배의 설정은 다음과 같다.
        ```json
        PUT _cluster/settings
        {
            "transient" : {
                "cluster.routing.use_adaptive_replica_selection" : true
            }
        }
        ```  

* 글로벌 타암아웃 설정
    * 개별 요청에 대한 타임 아웃은 Request Body에 할 수 있지만.
    * 모든 요청에 대해 타입 아웃을 설정 할 수 있다.
        * 기본은 무제한이다.
    * 글로벌 타임 아웃의 설정은 다음과 같다.
        ```json
        PUT _cluster/_settings
        {
            "transient" : {
                "search.default_search_timeout" : "1s"
            }
        }
        ```  

### 4.4.2 Search Shard API
* Search Shard API를 이용하면 검색이 수행되는 노드 및 샤드의 정보를 확인 할 수 있다.  
* 이러한 정보는 질의를 최적화 하거나, 질의가 정상적으로 수행되지 않을 때 문제를 해결 하는 유용한 수단이 된다.
* 인덱스 정보 확인은 다음과 같다.
    ```json
    POST movie_search/_search_shards
    ```
* 결과는 다음과 같다.
    ```json
    {
        "nodes" : {
            "ZQWpb3B8QRupZSxFAogeXQ" : {
                "name" : "node-1",
                "ephemeral_id" : "UN319m-TThyXxYRPXID4LQ",
                "transport_address" : "172.30.1.27:9300",
                "attributes" : {
                    "ml.machine_memory" : "12769456128",
                    "xpack.installed" : "true",
                    "ml.max_open_jobs" : "20",
                    "ml.enabled" : "true"
                }
            }
        },
        "indices" : {
            "movie_search" : {}
        },
        "shards" : [
            [
                {
                    "state" : "STARTED",
                    "primary" : true,
                    "node" : "ZQWpb3B8QRupZSxFAogeXQ",
                    "relocating_node" : null,
                    "shard" : 0,
                    "index" : "movie_search",
                    "allocation_id" : {
                        "id" : "ayyoNQrmRGaupT2hVIFiDQ"
                    }

                }
            ]
            .....
        ]
    }
    ```
<br/>

### 4.4.3 Multi Search API
* 여러 건의 검색 요청을 통합해서 한번에 요청하고 한번에 결과를 종합해서 받을 때 사용한다.
    ```json
    POST _msearch
    {"index" : "movie_auto"}
    {"query" : {"match_all" : {}}, "from" : 0, "size" : 10}
    {"index" : "movie_search"}
    {"query" : {"match_all" : {}}, "from" : 0, "size" : 10}
    ```
<br/>

### 4.4.4 Count API
* 검색된 문서의 개수만 가져올 때 사용한다.
* URI, Request Body 검색 모두 사용할 수 있다.
    ```json
    POST movie_serch/_count?q=prdtYear:2017

    // 결과
    {
        "count" : 4,
        "_shards" : {
            "total" : 5,
            "successful" : 5,
            "skipped" : 0,
            "failed" : 0
        }
    }
    ```
<br/>

